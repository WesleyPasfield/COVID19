{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19 Topic Modeling & Article Similarity Matching\n",
    "\n",
    "This Jupyter Notebook takes data published in the Kaggle COVID-19 challenge and creates a topic model on a subset of the data, and uses FAISS to find the most similar articles in the training set to an unseen article.\n",
    "\n",
    "The source data can be found here: https://www.kaggle.com/covid19\n",
    "\n",
    "Possible extensions are to scale up to the total dataset removing the random sampling of the dataset, improving the presentation of the results, and getting to a more granular similarity matching at the sentence level.\n",
    "\n",
    "First we'll import required packages, and install a few packages as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import ast\n",
    "import random\n",
    "pd.set_option('display.max_columns', 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/Wesley/anaconda3/envs/py37/lib/python3.7/site-packages (3.4.5)\n",
      "Requirement already satisfied: gensim in /Users/Wesley/anaconda3/envs/py37/lib/python3.7/site-packages (3.8.1)\n",
      "Requirement already satisfied: wordcloud in /Users/Wesley/anaconda3/envs/py37/lib/python3.7/site-packages (1.6.0)\n",
      "Requirement already satisfied: faiss-cpu in /Users/Wesley/anaconda3/envs/py37/lib/python3.7/site-packages (1.6.3)\n",
      "Requirement already satisfied: six in /Users/Wesley/anaconda3/envs/py37/lib/python3.7/site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/Wesley/anaconda3/envs/py37/lib/python3.7/site-packages (from gensim) (1.3.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/Wesley/anaconda3/envs/py37/lib/python3.7/site-packages (from gensim) (1.10.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /Users/Wesley/anaconda3/envs/py37/lib/python3.7/site-packages (from gensim) (1.16.4)\n",
      "Requirement already satisfied: matplotlib in /Users/Wesley/anaconda3/envs/py37/lib/python3.7/site-packages (from wordcloud) (3.2.1)\n",
      "Requirement already satisfied: pillow in /Users/Wesley/anaconda3/envs/py37/lib/python3.7/site-packages (from wordcloud) (6.1.0)\n",
      "Requirement already satisfied: boto3 in /Users/Wesley/anaconda3/envs/py37/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (1.12.31)\n",
      "Requirement already satisfied: requests in /Users/Wesley/anaconda3/envs/py37/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied: google-cloud-storage in /Users/Wesley/anaconda3/envs/py37/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (1.26.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/Wesley/anaconda3/envs/py37/lib/python3.7/site-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/Wesley/anaconda3/envs/py37/lib/python3.7/site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/Wesley/anaconda3/envs/py37/lib/python3.7/site-packages (from matplotlib->wordcloud) (2.8.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/Wesley/anaconda3/envs/py37/lib/python3.7/site-packages (from matplotlib->wordcloud) (1.2.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /Users/Wesley/anaconda3/envs/py37/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.5)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /Users/Wesley/anaconda3/envs/py37/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.31 in /Users/Wesley/anaconda3/envs/py37/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (1.15.31)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/Wesley/anaconda3/envs/py37/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (1.24.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/Wesley/anaconda3/envs/py37/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.6.16)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/Wesley/anaconda3/envs/py37/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/Wesley/anaconda3/envs/py37/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: google-resumable-media<0.6dev,>=0.5.0 in /Users/Wesley/anaconda3/envs/py37/lib/python3.7/site-packages (from google-cloud-storage->smart-open>=1.8.1->gensim) (0.5.0)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.11.0 in /Users/Wesley/anaconda3/envs/py37/lib/python3.7/site-packages (from google-cloud-storage->smart-open>=1.8.1->gensim) (1.12.0)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.2.0 in /Users/Wesley/anaconda3/envs/py37/lib/python3.7/site-packages (from google-cloud-storage->smart-open>=1.8.1->gensim) (1.3.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /Users/Wesley/anaconda3/envs/py37/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.31->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /Users/Wesley/anaconda3/envs/py37/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim) (4.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /Users/Wesley/anaconda3/envs/py37/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim) (41.0.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/Wesley/anaconda3/envs/py37/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/Wesley/anaconda3/envs/py37/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim) (4.0.0)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.16.0 in /Users/Wesley/anaconda3/envs/py37/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/Wesley/anaconda3/envs/py37/lib/python3.7/site-packages (from rsa<4.1,>=3.1.4->google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim) (0.4.8)\n",
      "Requirement already satisfied: pytz in /Users/Wesley/anaconda3/envs/py37/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim) (2019.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /Users/Wesley/anaconda3/envs/py37/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim) (1.51.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /Users/Wesley/anaconda3/envs/py37/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim) (3.11.3)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install nltk gensim wordcloud faiss-cpu\n",
    "\n",
    "import nltk\n",
    "import gensim\n",
    "import wordcloud\n",
    "import faiss\n",
    "from nltk.corpus import stopwords\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "Next let's load and process the articles. The first step will be to load the articles, which have been downloaded onto my local machine. Due to memory related issues, we'll randomly sample 20% of the articles from each source. This is not ideal, but the sample size created is still large (>3000 articles).\n",
    "\n",
    "These functions pull data sourced from my location machine, I've downloaded the data from Kaggle and am loading it that way. Not ideal for reproducability, but just change the file locations listed below if your files are in another location, and make sure there's no other data outside of the kaggle-provided data in those locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Files at the below location:\n",
      "biorxiv_medrxiv/biorxiv_medrxiv\n",
      "There are 177 files to process\n",
      "There were 885 files in the dataset\n",
      "Processing Files at the below location:\n",
      "noncomm_use_subset/noncomm_use_subset\n",
      "There are 470 files to process\n",
      "There were 2353 files in the dataset\n",
      "Processing Files at the below location:\n",
      "comm_use_subset/comm_use_subset\n",
      "There are 1823 files to process\n",
      "There were 9118 files in the dataset\n",
      "Processing Files at the below location:\n",
      "custom_license/custom_license\n",
      "There are 3391 files to process\n",
      "There were 16959 files in the dataset\n",
      "Example File Name\n",
      "5078f271f0b8d13359084747a8244563005d720a.json\n",
      "Number of files\n",
      "3391\n",
      "Example Article Information\n",
      "['biorxiv_medrxiv', 'Title: Feasibility of controlling 2019-nCoV outbreaks by isolation of cases and contacts', ['Evidence before this study Contact tracing and isolation of cases is a commonly used intervention for controlling infectious disease outbreaks. This intervention can be effective, but may require intensive public health effort and cooperation to effectively reach and monitor all contacts. When the pathogen has infectiousness before symptom onset, control of outbreaks using contact tracing and isolation is more challenging. This study uses a mathematical model to assess the feasibility of contact tracing and case isolation to control outbreaks of 2019-nCov, a newly emerged pathogen. We used disease transmission characteristics specific to the pathogen and therefore give the best available evidence if contact tracing and isolation can achieve control of outbreaks. Contact tracing and isolation may not contain outbreaks of 2019-nCoV unless very high levels of contact tracing are achieved. Even in this case, if there is asymptomatic transmission, or a high fraction of transmission before onset of symptoms, this strategy may not achieve control within three months. As of 5th February 2020, there have been over 24,550 confirmed cases of a novel coronavirus infection (2019-nCoV), including over 190 international cases, and over 490 reported deaths 1 . Control measures have been instigated within China to try to contain the outbreak 2 . As infectious people arrive in countries or areas without ongoing transmission, efforts are being made to halt transmission, and prevent potential outbreaks 3, 4 . Isolation of confirmed and suspected cases, and identification of contacts are a critical part of these control efforts. It is not yet clear if these efforts will achieve control of transmission of 2019-nCoV. Isolation of cases and contact tracing becomes less effective if infectiousness begins before the onset of symptoms 5, 6 . For example, the severe acute respiratory syndrome (SARS) outbreak that began in Southern China in 2003 was amenable to eventual control through tracing contacts of suspected cases and isolating confirmed cases because the majority of transmission occurred after symptom onset 7 . These interventions also play a major role in response to outbreaks where onset of symptoms and infectiousness are concurrent 10 , for example Ebola virus disease 8, 9 and MERS 10, 11 , and for many other infections 12, 13 . The effectiveness of isolation and contact tracing methods hinges on two key epidemiological parameters: the number of secondary infections generated by each new infection and the proportion of transmission that occurs prior to symptom onset 5 . In addition, the probability of successful contact tracing and the delay between symptom onset and isolation are critical, since cases remain in the community where they can infect others until isolation 6, 14 . Transmission prior to symptom onset could only be prevented by tracing contacts of confirmed cases and testing (and quarantining) those contacts. Cases that do not seek care, potentially due to subclinical or asymptomatic transmission represent a further challenge to control. If 2019-nCoV can be controlled by isolation and contact tracing, then public health efforts should be focussed on this strategy. However, if this is not enough to control outbreaks, then additional resources may be needed for additional interventions. There are currently key unknown characteristics of the transmissibility and natural history of 2019-nCoV; for example, whether transmission can occur before symptom onset. Therefore we explored a range of epidemiological scenarios that represent potential transmission properties based on current information about 2019-nCoV transmission. We assessed the ability of isolation and contact tracing to control disease outbreaks using a mathematical model 6, [15] [16] [17] [18] . By varying . CC-BY-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not peer-reviewed) The copyright holder for this preprint . https://doi.org/10.1101/2020.02.08.20021162 doi: medRxiv preprint the efficacy of contact tracing efforts, the size of the outbreak when detected, and the promptness of isolation after symptom onset, we show how viable it is for countries at risk of imported cases to use contact tracing and isolation as a containment strategy. We implemented a branching process model in which the number of potential secondary cases produced by each individual (the \\'infector\\') is drawn from a negative binomial distribution with a mean equal to the reproduction number, and heterogeneity in the number of new infections produced by each individual 6, 15, [18] [19] [20] . Each potential new infection was assigned a time of infection drawn from the serial interval distribution. Secondary cases were only created if the infector had not been isolated by the time of infection. In the example in Figure 1 , person A can potentially produce three secondary infections (because three is drawn from the negative binomial distribution), but only two transmissions occur before the case was isolated. Thus, a reduced delay from onset to isolation reduced the average number of secondary cases in the model. Figure 1 : Example of the simulated process that starts with person A being infected. After an incubation period (blue) person A shows symptoms and is isolated at a time drawn from the delay distribution (green) ( Table 1) . A draw from the negative binomial distribution with mean R0 and distribution parameter determines how many people person A potentially infects. For each of those, a serial interval is drawn (orange). Two of these exposures occur before the time that person A is isolated. With probability ρ, each contact is traced, with probability 1-ρ they are missed by contact tracing. Person B is successfully traced, which means that they will be isolated without a delay when they develop symptoms. hey could, however, still infect others before they are isolated. Person C is missed by contact tracing. This means that they are only detected if and when symptomatic, and are isolated after a delay from symptom onset. Because person C was not traced they infected two more people (E and F) in addition to person D than if they had been isolated at symptom onset. A version with asymptomatic transmission is given in Figure S8 . We initialised the branching process with 5, 20, or 40 cases to represent a newly detected outbreak of varying size. Initial symptomatic cases were then isolated after symptom onset with a delay drawn from the onset-to-isolation distribution (Table 1) . Isolation was assumed to be 100% effective at preventing further transmission; therefore, in the model, failure to control the outbreak resulted from the lack of complete contact tracing and the delays in isolating cases rather than the inability of isolation to prevent further transmission. Either 100% or 90% of cases became symptomatic, and all symptomatic cases were eventually reported. Each newly infected case was identified through contact tracing with probability ρ. Secondary cases that had been traced were isolated immediately upon becoming symptomatic. Cases that were missed by contact tracing (probability 1-ρ) were isolated when they became symptomatic with a delay drawn from the onset-to-isolation distribution. In addition, each case had an independent probability of being subclinical (asymptomatic), and were therefore not detected either by self report or if traced by contact tracing. New secondary cases caused by an asymptomatic case were missed by contact tracing and could only be isolated based on symptoms. The model includes isolation of symptomatic individuals only, i.e. no quarantine, so isolation cannot prevent transmission before symptom onset. Quarantining contacts of cases requires a considerable investment in public health resources, and has not been widely implemented for all contacts of cases 3,21 . We ran 1,000 simulations of each combination of the proportion of transmission before symptom onset, R0, onset-to-isolation delay, the number of initial cases, and the probability that a contact was traced (Table 1) . We explored two scenarios of delay between symptom onset and isolation: \"short\" and \"long\" . CC-BY-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint . https://doi.org/10.1101/2020.02.08.20021162 doi: medRxiv preprint The incubation period for each case was drawn from a Weibull distribution. A corresponding serial interval for each case was then drawn from a skewed normal distribution with the mean parameter of the distribution set to the incubation period for that case, a standard deviation of 2, and a skew parameter chosen such that a set proportion of serial intervals were shorter than the incubation period (meaning that a set proportion of transmission happened before symptom onset) ( Figure 2 ). This sampling approach ensured that the serial interval and incubation period for each case was correlated, and prevents biologically implausible scenarios where a case can develop symptoms very soon after exposure but not become infectious until very late after exposure and vice versa. There are many estimates of the reproduction number for the early phase of the 2019-nCoV outbreak in Wuhan, China 15, 18, 19, 23, 26, [26] [27] [28] [29] [30] , and therefore we used the values 1.5, 2.5, and 3.5, which span most of the range of current estimates (Table 1) . We used the secondary case is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint . https://doi.org/10.1101/2020.02.08.20021162 doi: medRxiv preprint short delay to isolation, 15% of transmission before symptom onset, and 0% subclinical infection. Table 1 . B) The incubation distribution estimate fitted to data from the Wuhan outbreak by Backer et al. 24 . C) An example of the method used to sample the serial interval for a case that has an incubation period of 5 days. Each case has an incubation period drawn from the distribution in B, their serial interval is then drawn from a skewed normal distribution with the mean set to the incubation period of the case. In C, the incubation period was 5 days. The skew parameter of the skewed normal distribution controls the proportion of transmission that occurs before symptom onset, the three scenarios explored are <1% of transmission before onset (grey), 15% of transmission before onset (gold), and 30% of transmission before onset (pink). Outbreak control was defined as no new infections between 12 and 16 weeks after the initial cases. Outbreaks that reached 5,000 cumulative cases were assumed to be too large to control within 12-16 weeks, and were categorised as uncontrolled outbreaks. Based on this definition, we reported the probability that an outbreak of a 2019-nCoV-like pathogen would be controlled within 12 weeks for each scenario, assuming that the basic reproduction number remained constant and no other interventions were implemented. The probability that an outbreak is controlled gives a one-dimensional understanding of the difficulty involved in achieving control, because the model places no constraints on the number of cases and contacts that can be traced and isolated. In reality, the feasibility of contact tracing and isolation is likely to be determined both by the probability of achieving control, and the resources needed to trace and isolate infected cases 31 . We therefore reported the weekly maximum number of cases undergoing contact tracing and isolation for each scenario that results in outbreak control. Once the weekly number of cases reaches a . CC-BY-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint . https://doi.org/10.1101/2020.02.08.20021162 doi: medRxiv preprint certain point, it can overwhelm the contact tracing system and affect the quality of the contact tracing effort 32 . All code is available as an R package (https://github.com/epiforecasts/ringbp). The funders of the study had no role in study design, data collection, data analysis, data interpretation, writing of the report, or the decision to submit for publication. All authors had full access to all the data in the study and were responsible for the decision to submit the manuscript for publication. To achieve 90% of outbreaks controlled required 80% of contacts to be traced and isolated for scenarios with a reproduction number of 2.5 (Figure 3a) . The probability of control was . CC-BY-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint . https://doi.org/10.1101/2020.02.08.20021162 doi: medRxiv preprint higher at all levels of contact tracing when the reproduction number was lower, and fell rapidly for a reproduction number of 3.5. At a reproduction number of 1.5, the effect of isolation is coupled with the chance of stochastic extinction resulting from overdispersion 20 , which is why some outbreaks were controlled even at 0% contacts traced. Isolation and contact tracing decreased transmission, as shown by a decrease in the effective reproduction number (Figure 3b ). For the scenario where the basic reproduction number was 1.5, the median estimate rapidly fell below 1, which indicates that control is likely. For the higher transmission scenarios a higher level of contact tracing was needed to bring the median effective reproduction number below 1. Figure 4 : The percentage of outbreaks controlled for the baseline scenario (black), and varied number of initial cases (A), time from onset to isolation (B), percentage of transmission before symptoms (C), and proportion of subclinical (asymptomatic) cases (D). The baseline scenario is R0 of 2.5, 20 initial cases, a short delay to isolation, 15% of transmission before symptom onset, and 0% subclinical infection. Results for R0 = 1.5 and 3.5 are given in the supplement. A simulated outbreak is defined as controlled if there are no cases between weeks 12 and 16 after the initial cases. The number of initial cases had a large impact on the probability of achieving control. With five initial cases, there was a greater than 50% chance of achieving control in 3 months, . CC-BY-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint . https://doi.org/10.1101/2020.02.08.20021162 doi: medRxiv preprint even at modest contact tracing levels (Figure 4a ). More than 40% of these outbreaks were controlled with no contact tracing due to the combined effects of isolation of symptomatic cases and stochastic extinction. The probability of control dropped as the number of initial cases increased, and for 40 initial cases, even 80% contact tracing did not lead to 80% of simulations controlled within 3 months. The delay from symptom onset to isolation played a major role in achieving control of outbreaks (Figure 4b ). At 80% of contacts traced, the probability of achieving control falls from 89% to 31% when there is a longer delay from onset to isolation. If there is no transmission before symptom onset then the probability of achieving control is higher for all values of contacts traced (Figure 4c) . The difference between 15% and 30% of transmission before symptoms had a marked effect on probability to control. We found this effect in all scenarios tested (supplementary Figure S4 ). Including only 10% of cases being asymptomatic resulted in a decreased probability that simulations were controlled by isolation and contact tracing for all values of contact tracing (Figure 4d ). For 80% of contacts traced, only 37% of outbreaks were controlled, compared with 89% without subclinical infection. In many scenarios there were between 25 and 100 symptomatic cases within a week ( Figure   5 ), all of whom would need isolation and would require contact tracing. The maximum number of weekly cases may appear counterintuitive because a lower maximum number of weekly cases is not associated with higher outbreak control. This occurs because with better contact tracing it becomes possible to control outbreaks with higher numbers of weekly cases. The maximum number of weekly cases is lower if the initial number of cases is 5 and higher if it is 40 (see supplement). In the 2014 Ebola epidemic in Liberia, each case reported between 6 and 20 contacts 8 , and the number of contacts may be higher, as seen in MERS outbreaks 10 . Tracing 20 contacts per case could mean up to 2,000 contacts in the week of peak incidence. . CC-BY-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint . https://doi.org/10.1101/2020.02.08.20021162 doi: medRxiv preprint Figure 5 : The maximum weekly cases requiring contact tracing and isolation in scenarios with 20 index cases that achieved control within 3 months. Scenarios vary by reproduction number and the mean delay from onset to isolation. 15% of transmission occurred before symptom onset, and 0% subclinical infection. The percentage of simulations that achieved control is shown in the boxplot. This illustrates the potential size of the eventually controlled simulated outbreaks, which would need to be managed through contact tracing and isolation. * indicates that the 95% interval extends out of the plotting region. We determined conditions where case isolation, contact tracing, and preventing transmission by infected contacts would be sufficient to control a new 2019-nCoV outbreak in the absence of other control measures. We found that in many plausible scenarios, case isolation alone would be unlikely to control transmission within three months. Case isolation was more effective when there was little transmission before symptom onset and when the delay from symptom onset to isolation was shorter. Preventing transmission by tracing and isolating a larger proportion of contacts, thereby decreasing the effective reproduction number, improved the number of scenarios where control was likely to be achieved. However, these outbreaks required a large number of cases to be contact traced and isolated each week, which is of crucial concern when assessing the feasibility of this strategy. Subclinical infection markedly decreased the probability of controlling outbreaks within 3 months. In scenarios where the reproduction number was 2.5, 15% of transmission occurred before symptom onset, and there was a short delay to isolation, at least 80% of infected contacts . CC-BY-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint . https://doi.org/10.1101/2020.02.08.20021162 doi: medRxiv preprint needed to be traced and isolated to give a probability of control of 90% or more. This echoes other suggestions that highly effective contact tracing will be necessary to control outbreaks in other countries 17 . In scenarios where the delay from onset to isolation was larger, similar to the delays seen in the early phase of the outbreak in Wuhan, the same contact tracing success rate of 80% achieved less than 40% probability of containing an outbreak. Higher pre-symptomatic transmission decreases the probability that the outbreaks were controlled, under all reproduction numbers and isolation delay distributions tested. Our model does not include other control measures that may decrease the reproduction number and therefore also increase the probability of achieving control of an outbreak. At the same time, it assumes that isolation of cases and contacts is completely effective, and that all symptomatic cases are eventually reported. Relaxing these assumptions would decrease the probability that control is achieved. We also make the assumption that contact is required for transmission between two individuals, whereas transmission via fomites may be possible. This would make effective contact tracing challenging, and good respiratory and hand hygiene would be critical to reduce this route of transmission, coupled with environmental decontamination in healthcare settings. We intentionally simplified our model to determine the effect of contact tracing and isolation on the control of outbreaks under different scenarios of transmission. However, as more data becomes available, the model can be updated, or tailored to particular public health contexts. It is likely that the robustness of control measures is affected both by differences in transmission between countries but also by the concurrent number of cases that require contact tracing in each scenario. Practically, there is likely to be an upper bound on the number of cases that can be traced, and case isolation is likely to be imperfect 33 . We reported the maximum number of weekly cases during controlled outbreaks but the capacity of response efforts may vary. We explored a range of scenarios informed by the latest evidence on transmission of 2019-nCoV. Similar analyses using branching models have already been used to analyse the Wuhan outbreak to find plausible ranges for the initial exposure event size and the basic reproduction number 15, 19 . Our analysis expands on this by including infectiousness before the onset of symptoms, case isolation, explicit modelling of case incubation periods and time to infectiousness. A key area of uncertainty is if and for how long individuals are infectious before symptom onset, and if asymptomatic or subclinical infection occurs. Both are likely to make the outbreak harder to control 24 . The model could be modified to include some transmission after isolation (such as in hospitals) which would decrease the probability of achieving control. In addition, we define an outbreak as controlled if it reaches extinction by 3 months, regardless of outbreak size or number of weekly cases. This definition may be narrowed where the goal is to keep the overall caseload of the outbreak low. This may be of concern to both local authorities for reducing the healthcare surges, and may provide a way to limit geographic spread. Our study indicates that in most plausible outbreak scenarios case isolation and contact tracing alone is insufficient to control outbreaks, and that in certain scenarios even near perfect contact tracing will still be insufficient, and therefore further interventions would be required to achieve control. However, rapid and effective contact tracing can also reduce the initial number of cases, which would make the outbreak easier to control overall. Effective contact tracing and isolation could contribute to reducing the overall size of an outbreak or bringing it under control over a longer time period. . CC-BY-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.  The authors have no interests to declare. No data were used in this study. The R code for the work is available at https://github.com/epiforecasts/ringbp. . CC-BY-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not peer-reviewed) The copyright holder for this preprint . https://doi.org/10.1101/2020.02.08.20021162 doi: medRxiv preprint']]\n"
     ]
    }
   ],
   "source": [
    "file_locations = ['biorxiv_medrxiv/biorxiv_medrxiv', \n",
    "                  'noncomm_use_subset/noncomm_use_subset', \n",
    "                  'comm_use_subset/comm_use_subset',\n",
    "                  'custom_license/custom_license']\n",
    "\n",
    "## Set up Stop Words\n",
    "## Add Any other relevant options manually\n",
    "\n",
    "stop_words = stopwords.words('english') + ['et', 'al', 'fig', 'etal', 'et al', 'et-al']\n",
    "\n",
    "processed_articles = process_articles(file_locations, stop_words)\n",
    "processed_articles.read_files()\n",
    "\n",
    "print('Example File Name')\n",
    "print(processed_articles.root_files[0])\n",
    "print('Number of files')\n",
    "print(len(processed_articles.root_files))\n",
    "print('Example Article Information')\n",
    "print(processed_articles.title_text[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's process the text - this involves removing links and other undesirable text, tokenizing words from the raw paragraph text, removing stop words (the, it etc...) and finally creating stems of words to group derivatives of the same root word together.\n",
    "\n",
    "There's no contextual meaning associated with these stems, which is why word embedding based approaches have grown in popularity as the vectors they create attempt to capture the contextual meaning of words based on the words around them rather than just represent the raw text. Shifting to that type of approach is a logical extension of this initial work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning out Junk\n",
      "Tokenizing words\n",
      "Converting to list of words and removing stop words\n",
      "Creating word stems\n"
     ]
    }
   ],
   "source": [
    "processed_articles.process_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll create bigrams and trigrams of the data - this concatenates words that are used frequently with one and other to create their own representation. As an example, *multicellular_eukaryot* is a bigram create based on these two words appearing frequently in the text corpus. *oil_immers_object* is a trigram created for the same reason, the only difference is an additional word on the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_articles.trigrams([art[3] for art in processed_articles.processed_article])\n",
    "processed_articles.processed_trigrams[2][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's split the data into train and test, in this case the test data is used to compute perplexity as well as to have a held out set for similarity matching used later. The model with the lowest perplexity is used as the final model. Topic Coherence is another method used to evaluate the quality of topic creation, but that was not pursued in this example.\n",
    "\n",
    "Often topics are manually reviewed to determine the appropriate number of topics, but in this case I don't have enough subject matter expertise to provide any value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "train, test = train_test_splitter(processed_articles.processed_trigrams, 0.90)\n",
    "\n",
    "vectorizer = CountVectorizer(min_df = 50, max_df = 0.8, max_features = 50000)\n",
    "tf = vectorizer.fit_transform([t[4] for t in train]) ## Vectorize training set\n",
    "tf_feature_names = vectorizer.get_feature_names() ## Pull out words for use in eval\n",
    "\n",
    "# Transform test data for perplexity eval\n",
    "\n",
    "tf_test = vectorizer.transform([t[4] for t in test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling - LDA\n",
    "\n",
    "Next let's build the topic model. We'll use Latent Dirichlet Allocation (LDA), specifically the implementation from sklearn. The key concept of LDA is that there are latent topics that exist within a document corpus based on words used in each document. LDA is an unsupervised probablistic model where documents are probability distributions over latent topics, and topics are probability distributions over words. In simplistic terms, LDA aims to find collections of words that are disproportionately represented within documents relative to the total document corpus to identify latent topics.\n",
    "\n",
    "LDA is not a supervised model, a practitioner provides processed text and a number of topics, and LDA creates the specified number of topics based on observed probability distributions of words within documents. LDA is a bag of words model evaluated within each document, so the order of words is immaterial.\n",
    "\n",
    "For each document LDA assumes all topic assignments are correct except for the document in question. From there, it calculates two proportions. The first is the proportion of words in the current document that are assigned to a specific topic. Then the proportion of assignments to that topic over all documents that come from each word. These proportions get multiplied across all words and topics to update the probabilities that a word is assigned to a topic.\n",
    "\n",
    "LDA trades off two adversarial goals to find the appropriate distributions. The first is in each document it wants to allocate words to as few topics as possible, the second is for each topic it wants to assign high probability to few words.\n",
    "\n",
    "The end outcome is each document receives a distribution of scores across each topic that sums to 1. The higher the score for a particular topic, the more representative a topic is of that document. We can also evaluate each topic to see which words are most prevalent in total, and relative to how prevalent they are relative to total word usage in the overall corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "perps = []\n",
    "models = []\n",
    "\n",
    "# Below was used for testing to find optimal number of topics based on test data perplexity. \n",
    "# Commented out for brevity's sake in the notebook - using number of topics with lowest perplexity from testing\n",
    "\n",
    "'''\n",
    "\n",
    "for num_top in range(20,40,1):\n",
    "    \n",
    "    #'online': Online variational Bayes method. In each EM update, use\n",
    "    #mini-batch of training data to update the ``components_``\n",
    "    #variable incrementally.\n",
    "    # Controlled by learning offset and decay functions\n",
    "    # Offset & decay good candidates for optimization\n",
    "    \n",
    "    lda = LatentDirichletAllocation(n_components=num_top,\n",
    "                                    learning_method = 'online',\n",
    "                                    verbose = 1,\n",
    "                                    learning_offset = 15., # Downweights early iterations\n",
    "                                    learning_decay = 0.75, # default = 0.7\n",
    "                                    random_state = 100\n",
    "                                   )\n",
    "    print(f\"{num_top} topics\")\n",
    "    print('fitting')\n",
    "    ldamod = lda.fit(tf)\n",
    "    perp = ldamod.perplexity(tf_test)\n",
    "    print(f'Perplexity for {num_top}')\n",
    "    print(perp)\n",
    "    perps.append(perp)\n",
    "    models.append(ldamod)\n",
    "\n",
    "'''\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=32,\n",
    "                                    learning_method = 'online',\n",
    "                                    verbose = 1,\n",
    "                                    learning_offset = 15., # Downweights early iterations\n",
    "                                    learning_decay = 0.75, # default = 0.7\n",
    "                                    random_state = 100\n",
    "                                   )\n",
    "\n",
    "ldamod = lda.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalinfo = LDA_Evaluator(lda_model = ldamod, vectorizer = vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's look at the word frequency within a topic to get a better understanding of topic composition. The first table shows the top 20 words in terms of frequency within topic 3. The second shows the top 20 words in terms of frequency relative to all other topics. Both of these are based off the components_ provided by the LDA implementation, which is described as a \"pseudocount that represents the number of times word j was assigned to topic i.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate words that are highest per topic\n",
    "# Use topic three as an example \n",
    "\n",
    "evalinfo.eval_raw_frequency(3, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate the words that show up the most relative to other topics for each topic\n",
    "\n",
    "evalinfo.eval_rel_frequency(3, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wc = wcEval(train, vectorizer, ldamod)\n",
    "train_wc.raw_freq_wc()\n",
    "train_wc.rel_freq_wc()\n",
    "\n",
    "test_wc = wcEval(test, vectorizer, ldamod)\n",
    "\n",
    "train_topic_fin_raw = [[t[0], t[2], primary_top] for t, primary_top in zip(train, train_wc.raw_primary_topic)]\n",
    "train_topic_fin_rel = [[t[0], t[2], rel_primary_top] for t, rel_primary_top in zip(train, train_wc.rel_primary_topic)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be challenging to interpret topics this way, especially when the number of topics is larger. Another approach is to create a word cloud that displays the most frequently used words in a corpus in a visual way. We'll create a word cloud for each topic by assigning a document to the topic that it has the highest score relative to all other topics. To determine relative score, we looked at the value of topic n divided by the average value for topic n across all topics, selecting the topic with the highest value. Certain topics have higher scores in general based on probablistic assignment, so this approach aims to produce a more even distribution of documents across topics.\n",
    "\n",
    "In addition to generating the word cloud we also show the highest relative frequency words within that topic, as the word cloud itself will disproportionately weight higher frequency terms when generated. The relative frequency words underneath aim to provide more clarity into the topic.\n",
    "\n",
    "Looking through the word clouds there appears to be some apparent themes emerging, with some topics focused on data and clinical observations, while others are focused on specific genomic sequences, among other themes. As a lay-person I cannot make much sense of some of the differences, but the hope would be that an expert could, and creating this topic distribution helps them interpret quickly what a new document contains based simply on its topic assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "\n",
    "for t in np.arange(32):\n",
    "    #try:\n",
    "    word_clouds(train_topic_fin_rel, t, 200, stop_words, evalinfo)\n",
    "    #except:\n",
    "        #print('failure')\n",
    "        #print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Matching\n",
    "\n",
    "Finally let's create a mechanism to take a new document and find the 10 most similar documents in our training database. We'll use the FAISS package to create a fast indexing system. Faiss is a library for efficient similarity search and clustering of dense vectors that came out of Facebook Research: https://github.com/facebookresearch/faiss\n",
    "\n",
    "We'll use the topic scores generated by the topic model as the dense vectors to pass into the faiss index. Then after scoring a new document with it's topic scores, we'll look up the most similar documents we have available based on cosine similarity. For brevity's sake we'll just return the topic score of the new document, the document title, and the document titles of the most similar articles. A lay-person's quick review of these items seems to indicate the titles returned often match up closely with the new document provided. The similarity score shown is the cosine similarity.\n",
    "\n",
    "An extension of this work could be to do the same at the sentence or paragraph level to find specific areas of similarity rather than look at the full document level. This could provide value for researchers looking for very specific topics, rather than general similarities. In this case a topic model based approach could suffice, where paragraphs became documents rather than sentences, but topic models typically are not as useful on smaller texts. Instead, using an approach like BERT do create sentence or paragraph embeddings would be a logical place to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS - Create index based on training topic scores\n",
    "\n",
    "ncentroids = 100\n",
    "k = 4\n",
    "matrix_conv = np.ascontiguousarray(train_wc.topic_scores.astype('float32'))\n",
    "dim_index = len(matrix_conv[0])\n",
    "faiss.normalize_L2(matrix_conv)\n",
    "quantizer = faiss.IndexFlatL2(dim_index)  # the other index\n",
    "index = faiss.IndexIVFFlat(quantizer, dim_index, ncentroids, faiss.METRIC_INNER_PRODUCT)\n",
    "assert not index.is_trained\n",
    "index.train(matrix_conv)\n",
    "assert index.is_trained\n",
    "\n",
    "index.add(matrix_conv)                  # add may be a bit slower as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test data matrix, randomly sample from test set, and return 10 most similar candidates that have document titles\n",
    "\n",
    "\n",
    "test_matrix = np.ascontiguousarray(test_wc.topic_scores.astype('float32'))\n",
    "\n",
    "testD, testI = index.search(test_matrix, 50)\n",
    "rand = random.randint(0, (len(test) - 1))\n",
    "\n",
    "print(f\"Topic Scores for new article: \\n {test_wc.topic_scores.iloc[rand]}\")\n",
    "print(f\"Title of new article: \\n{test[rand][1]}\\n\")\n",
    "print(\"Most Similar Articles:\\n\")\n",
    "counter = 0\n",
    "valid = 1\n",
    "\n",
    "for val in testI[rand]:\n",
    "    counter = counter + 1\n",
    "    if valid > 10:\n",
    "        break\n",
    "    if test[rand][1] == '':\n",
    "        break\n",
    "    if train[val][1] == test[rand][1]:\n",
    "        continue\n",
    "    elif train[val][1] == '':\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"{valid}) Similarity Score = {np.around(testD[rand][counter], 3)}\\nTitle: {train[val][1]} \\nPublication: {train[val][0]}\\n\")\n",
    "        valid = valid + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible Extensions\n",
    "\n",
    "1. Scale up to avoid random sampling\n",
    "2. BERT - Find similar sentences or paragraphs rather than topics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py37)",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
